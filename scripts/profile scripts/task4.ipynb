{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "We have 4 paths here,\n",
    "\n",
    "    1. path_dir is the source where all the sub directories are located\n",
    "    2. src_dir is the source for text input files\n",
    "    3. dst_tmp_ir is destination path for temporary csv files\n",
    "    4. dst_dir is the destination path for final csv files\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = r'C:\\Users\\ydjoe\\Documents\\nlp\\personalized-dialog-dataset\\small'\n",
    "src_dir = os.path.join(path_dir,'txt files')\n",
    "dst_tmp_dir = os.path.join(path_dir,'csv files temp')\n",
    "dst_dir = os.path.join(path_dir,'csv files')\n",
    "\n",
    "lst_files = os.listdir(src_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Files in the Source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 personalized-dialog-task1-API-calls-dev.txt\n",
      "1 personalized-dialog-task1-API-calls-trn.txt\n",
      "2 personalized-dialog-task1-API-calls-tst-OOV.txt\n",
      "3 personalized-dialog-task1-API-calls-tst.txt\n",
      "4 personalized-dialog-task2-API-refine-dev.txt\n",
      "5 personalized-dialog-task2-API-refine-trn.txt\n",
      "6 personalized-dialog-task2-API-refine-tst-OOV.txt\n",
      "7 personalized-dialog-task2-API-refine-tst.txt\n",
      "8 personalized-dialog-task3-options-dev.txt\n",
      "9 personalized-dialog-task3-options-trn.txt\n",
      "10 personalized-dialog-task3-options-tst-OOV.txt\n",
      "11 personalized-dialog-task3-options-tst.txt\n",
      "12 personalized-dialog-task4-info-dev.txt\n",
      "13 personalized-dialog-task4-info-trn.txt\n",
      "14 personalized-dialog-task4-info-tst-OOV.txt\n",
      "15 personalized-dialog-task4-info-tst.txt\n",
      "16 personalized-dialog-task5-full-dialogs-dev.txt\n",
      "17 personalized-dialog-task5-full-dialogs-trn.txt\n",
      "18 personalized-dialog-task5-full-dialogs-tst-OOV.txt\n",
      "19 personalized-dialog-task5-full-dialogs-tst.txt\n"
     ]
    }
   ],
   "source": [
    "iter_tp = 0\n",
    "for iter_fl in lst_files:\n",
    "    print(iter_tp,iter_fl)\n",
    "    iter_tp+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ydjoe\\Documents\\nlp\\personalized-dialog-dataset\\full\\txt files\\personalized-dialog-task4-info-tst-OOV.txt\n"
     ]
    }
   ],
   "source": [
    "fl_no = 14\n",
    "path_final = os.path.join(src_dir,lst_files[fl_no])\n",
    "print(path_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating temp CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` temp csv file contains profile,knowldgebase as seperate columns i.e we are yet to add tags such as <user> <profile> <query> <system> etc.. ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11998, 7)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([])\n",
    "\n",
    "## Initializing df to add data to\n",
    "\n",
    "path_to_use = path_final\n",
    "ct = 0\n",
    "\n",
    "\n",
    "with open(path_to_use) as topo_file:\n",
    "\n",
    "    ## Open the text file from given path\n",
    "\n",
    "    cnt = 0\n",
    "    cnt_sub = 0\n",
    "\n",
    "    ## cnt_sub is a Counter \n",
    "\n",
    "    ar_3,ar_4 = [],[]\n",
    "    ar_1,ar_2 = [],[]\n",
    "    ar_6 = []\n",
    "    ar_7 = {}\n",
    "\n",
    "    ## we are initializing 4 arrays, each array stores the following\n",
    "    ## Gender, age-group , Call , Response , Counter\n",
    "    ## Gender (ar_1) and age-group (ar_2) are self explainable\n",
    "    ## Call has the question asked (ar_3)\n",
    "    ## Response is the answer given (ar_4)\n",
    "    ## Knowledge base has the restaurant details (ar_6)\n",
    "    ## Details has the details about restaurant, i.e ratings,location,number (ar_7)\n",
    "    ## Counter is to understand where a new conversation starts\n",
    "\n",
    "\n",
    "    for line in topo_file:\n",
    "        ct+=1\n",
    "\n",
    "        line = line.strip()    \n",
    "        if not line.strip(): continue\n",
    "\n",
    "        init_num = re.findall(r'\\d+',line)        \n",
    "\n",
    "        if init_num[0]=='1':\n",
    "\n",
    "            line_splits = line[1:].split()\n",
    "\n",
    "\n",
    "            ar_1.append(line_splits[0])\n",
    "\n",
    "            \n",
    "            ar_2.append(line_splits[1])\n",
    "\n",
    "\n",
    "            ar_5 = cnt\n",
    "\n",
    "\n",
    "\n",
    "            cnt+=1\n",
    "\n",
    "            if len(ar_3)==0: continue\n",
    "\n",
    "            ## Check if we are on line 1 of 1st conversation.\n",
    "            ## if we are skip for now\n",
    "\n",
    "            ar_6 = np.unique(np.array(ar_6)).tolist()\n",
    "            ar_6 = \" \".join(ar_6)\n",
    "            ar_6 = ' <KB> ' + ar_6 \n",
    "            ar_6 = [ar_6]*(len(ar_3)-1)\n",
    "            ar_6.insert(0,'')\n",
    "\n",
    "            ## Knowldge base Contains unique restaurant names \n",
    "            ## first we split the the lines that start with retro into 2\n",
    "            ## get unique items in the first part of splits array.\n",
    "            ## combine all these unique elements into a string with a space\n",
    "            ## add <kB> tag at the begininng \n",
    "            ## since we have to append it to all lines in csv file multiply it with size of conversation\n",
    "\n",
    "\n",
    "            ar_7 = ['<' + str(i) + '> ' + str(ar_7[i]) + '' for i in ar_7]            \n",
    "            ar_6 = np.unique(np.array(ar_6)).tolist()\n",
    "            ar_7 = \" \".join(ar_7)\n",
    "            ar_7 = ' ' + ar_7 + ' '\n",
    "            ar_7 = [ar_7]*(len(ar_3)-1)\n",
    "            ar_7.insert(0,'')   \n",
    "\n",
    "            ## There are id's and values about restaurants in task4\n",
    "            ## like {ratings_id : 3}, {location : location_api}\n",
    "            ## We split the the lines and add <,> for id and value after that\n",
    "            ## get unique items and combine all these unique elements into a string with a space\n",
    "            ## since we have to append it to all lines in csv file multiply it with size of conversation\n",
    "            \n",
    "            data = list(\n",
    "                zip(\n",
    "                    [ar_1[0]]*len(ar_3),\n",
    "                    [ar_2[0]]*len(ar_3),                    \n",
    "                    ar_3,\n",
    "                    ar_4,\n",
    "                    [ar_5]*len(ar_3),\n",
    "                    ar_6,\n",
    "                    ar_7       \n",
    "                    ))\n",
    "\n",
    "            df = df.append(data,ignore_index=True)\n",
    "            cnt_sub = 0\n",
    "            ar_3,ar_4,ar_6,ar_7 = [],[],[],{}\n",
    "            ar_1.pop(0)\n",
    "            ar_2.pop(0)\n",
    "            continue\n",
    "\n",
    "            ## Add the data of first Conversation to the main dataframe defined\n",
    "            ## ReSet the counter, Call and Response arrays\n",
    "            ## Since Gender and age-group are single values pop them from begining , essentially resetting them  \n",
    "\n",
    "\n",
    "        line_splits = line[len(str(init_num[0]))+1:].split('\\t')\n",
    "        \n",
    "        if len(line_splits)!=2:\n",
    "            line_splits = line[len(str(init_num[0]))+1:].split(' ')            \n",
    "            line_splits = list(filter(None, line_splits))\n",
    "            \n",
    "            ar_6.append(line_splits[0])\n",
    "            \n",
    "            ar_7[line_splits[1]] = line_splits[2]\n",
    "\n",
    "            continue   \n",
    "        ar_3.append(line_splits[0])\n",
    "        ar_4.append(line_splits[1])\n",
    "\n",
    "        ## Splitting Call and Response\n",
    "        ## Addinig Call and Response to arrays 3 & 4\n",
    "\n",
    "        cnt_sub+=1\n",
    "\n",
    "df = df.rename(columns={\n",
    "     0: 'Gender',\n",
    "     1: 'Age',\n",
    "     2: 'Call',\n",
    "     3: 'Response',\n",
    "     4: 'Count',\n",
    "     5: 'KB',\n",
    "     6: 'Details'\n",
    "     })\n",
    "\n",
    "## Adding Column Names to our dataframe \n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "fl_dst = lst_files[fl_no][:-3] + 'csv'\n",
    "path_dst_tmp = os.path.join(dst_tmp_dir,fl_dst)\n",
    "\n",
    "## getting file name and appending it to the destination path\n",
    "\n",
    "df.to_csv(path_dst_tmp,index=False)\n",
    "## Writing File to temp csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11998, 2)\n"
     ]
    }
   ],
   "source": [
    "TAGS = [\n",
    "\n",
    "    '<profile> ',\n",
    "    ' <context>',\n",
    "    ' <user> ',\n",
    "    ' <system> ',\n",
    "    ' <query> ']\n",
    "\n",
    "c = 0\n",
    "mn_ar_1 = []\n",
    "mn_ar_2 = []\n",
    "mn_ar_3 = []\n",
    "prev_count = 0\n",
    "prev_string = ''\n",
    "for i in range(len(df)):    \n",
    "    c+=1    \n",
    "\n",
    "    if df.iloc[i][4]!=prev_count:\n",
    "        prev_string = ''\n",
    "        prev_count = df.iloc[i][4]\n",
    "    \n",
    "    a2 = df.iloc[i][2]\n",
    "    a3 = df.iloc[i][3]    \n",
    "    call_string = prev_string + TAGS[4]+ a2    \n",
    "    prev_string = prev_string + TAGS[2]+  a2 + TAGS[3] + a3\n",
    "\n",
    "    to_append = TAGS[0]+ line\n",
    "    mn_ar_1.append(call_string)\n",
    "    mn_ar_2.append(a3)\n",
    "\n",
    "ndf = pd.DataFrame(            \n",
    "            list(zip(\n",
    "                mn_ar_1,mn_ar_2                  \n",
    "                )),\n",
    "\n",
    "            columns =[\n",
    "                'Call',\n",
    "                'Response'\n",
    "                ]\n",
    "                ) \n",
    "\n",
    "df['Profile'] = df['Gender'] + ' ' +df['Age']\n",
    "ndf['Call'] = TAGS[0]+ df['Profile'] + df['KB'] + df['Details'] + TAGS[1] + ndf['Call']\n",
    "\n",
    "print(ndf.shape)\n",
    "fl_dst = lst_files[fl_no][:-3] + '.csv'\n",
    "path_dst = os.path.join(dst_dir,fl_dst)\n",
    "ndf.to_csv(path_dst,index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the above steps as a Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_df(path_final):\n",
    "    df = pd.DataFrame([])\n",
    "    path_to_use = path_final\n",
    "    ct = 0\n",
    "    with open(path_to_use) as topo_file:\n",
    "        cnt = 0\n",
    "        cnt_sub = 0\n",
    "        ar_3,ar_4 = [],[]\n",
    "        ar_1,ar_2 = [],[]\n",
    "        ar_6 = []\n",
    "        ar_7 = {}\n",
    "\n",
    "\n",
    "        for line in topo_file:\n",
    "            #if ct>200: break\n",
    "            ct+=1\n",
    "\n",
    "            line = line.strip()    \n",
    "            if not line.strip(): continue\n",
    "\n",
    "            init_num = re.findall(r'\\d+',line)        \n",
    "\n",
    "            if init_num[0]=='1':\n",
    "\n",
    "                #print(line)\n",
    "                line_splits = line[1:].split()\n",
    "                ar_1.append(line_splits[0])\n",
    "                ar_2.append(line_splits[1])\n",
    "                ar_5 = cnt\n",
    "                cnt+=1\n",
    "\n",
    "                if len(ar_3)==0: continue\n",
    "                #print(ar_6)\n",
    "                ar_6 = np.unique(np.array(ar_6)).tolist()\n",
    "                ar_6 = \" \".join(ar_6)\n",
    "                ar_6 = ' <KB> ' + ar_6 \n",
    "                ar_6 = [ar_6]*(len(ar_3)-1)\n",
    "                ar_6.insert(0,'')\n",
    "\n",
    "                ar_7 = ['<' + str(i) + '> ' + str(ar_7[i]) + '' for i in ar_7]            \n",
    "                ar_6 = np.unique(np.array(ar_6)).tolist()\n",
    "                ar_7 = \" \".join(ar_7)\n",
    "                ar_7 = ' ' + ar_7 + ' '\n",
    "                ar_7 = [ar_7]*(len(ar_3)-1)\n",
    "                ar_7.insert(0,'')   \n",
    "\n",
    "                #print(ar_6)\n",
    "                #print(np.unique(np.array(ar_6)))\n",
    "                \n",
    "                data = list(\n",
    "                    zip(\n",
    "                        [ar_1[0]]*len(ar_3),\n",
    "                        [ar_2[0]]*len(ar_3),                    \n",
    "                        ar_3,\n",
    "                        ar_4,\n",
    "                        [ar_5]*len(ar_3),\n",
    "                        ar_6,\n",
    "                        ar_7       \n",
    "                        ))\n",
    "\n",
    "                df = df.append(data,ignore_index=True)\n",
    "                cnt_sub = 0\n",
    "                ar_3,ar_4,ar_6,ar_7 = [],[],[],{}\n",
    "                ar_1.pop(0)\n",
    "                ar_2.pop(0)\n",
    "                continue\n",
    "\n",
    "            line_splits = line[len(str(init_num[0]))+1:].split('\\t')\n",
    "            # print(line)\n",
    "            # print(len(str(init_num[0])))\n",
    "            # print(line[len(str(init_num[0]))+1:])\n",
    "            # print(line_splits)\n",
    "            if len(line_splits)!=2:\n",
    "                line_splits = line[len(str(init_num[0]))+1:].split(' ')            \n",
    "                line_splits = list(filter(None, line_splits))\n",
    "                #print(line_splits)\n",
    "                ar_6.append(line_splits[0])\n",
    "                #print(line_splits)\n",
    "                ar_7[line_splits[1]] = line_splits[2]\n",
    "                #print(line)\n",
    "                #print(line_splits[0])\n",
    "                continue   \n",
    "            ar_3.append(line_splits[0])\n",
    "            ar_4.append(line_splits[1])\n",
    "            cnt_sub+=1\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        0: 'Gender',\n",
    "        1: 'Age',\n",
    "        2: 'Call',\n",
    "        3: 'Response',\n",
    "        4: 'Count',\n",
    "        5: 'KB',\n",
    "        6: 'Details'\n",
    "        })\n",
    "    print(df.shape)\n",
    "    fl_dst = lst_files[fl_no][:-3] + 'csv'\n",
    "    path_dst_tmp = os.path.join(dst_tmp_dir,fl_dst)\n",
    "    df.to_csv(path_dst_tmp,index=False)\n",
    "    return df\n",
    "\n",
    "TAGS = [\n",
    "\n",
    "    '<profile> ',\n",
    "    ' <context>',\n",
    "    ' <user> ',\n",
    "    ' <system> ',\n",
    "    ' <query> ']\n",
    "    \n",
    "def add_context(df):\n",
    "    prev_count = 0\n",
    "    txt = []\n",
    "    for i in range(len(df)):    \n",
    "        if df.iloc[i][4]!=prev_count:\n",
    "            prev_count = df.iloc[i][4]\n",
    "            txt.append('')\n",
    "            continue\n",
    "        txt.append(' <context>')  \n",
    "    df['TAG'] = txt\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_from_df(df):\n",
    "    c = 0\n",
    "    mn_ar_1 = []\n",
    "    mn_ar_2 = []\n",
    "    mn_ar_3 = []\n",
    "    prev_count = 0\n",
    "    prev_string = ''\n",
    "    for i in range(len(df)):    \n",
    "        #if c > 100: break\n",
    "        c+=1    \n",
    "\n",
    "        if df.iloc[i][4]!=prev_count:\n",
    "            prev_string = ''\n",
    "            prev_count = df.iloc[i][4]\n",
    "        \n",
    "        a2 = df.iloc[i][2]\n",
    "        a3 = df.iloc[i][3]    \n",
    "        call_string = prev_string + TAGS[4]+ a2    \n",
    "        prev_string = prev_string + TAGS[2]+ a2 + TAGS[3] + a3\n",
    "        # print(a2)\n",
    "        # print('call: ',call_string)\n",
    "        # print('prev: ',call_string)\n",
    "        # print(a3)\n",
    "        mn_ar_1.append(call_string)\n",
    "        mn_ar_2.append(a3)\n",
    "\n",
    "    ndf = pd.DataFrame(            \n",
    "                list(zip(\n",
    "                    mn_ar_1,mn_ar_2                  \n",
    "                    )),\n",
    "\n",
    "                columns =[\n",
    "                    'Call',\n",
    "                    'Response'\n",
    "                    ]\n",
    "                    ) \n",
    "\n",
    "    df['Profile'] = df['Gender'] + ' ' +df['Age']\n",
    "    ndf['Call'] = TAGS[0]+ df['Profile'] + df['KB'] + df['Details'] + TAGS[1] + ndf['Call']\n",
    "\n",
    "    print(ndf.shape)\n",
    "    fl_dst = lst_files[fl_no][:-3] + '.csv'\n",
    "    path_dst = os.path.join(dst_dir,fl_dst)\n",
    "    ndf.to_csv(path_dst,index=False)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A loop to get the index of Files to run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 C:\\Users\\ydjoe\\Documents\\nlp\\personalized-dialog-dataset\\small\\txt files\\personalized-dialog-task3-options-tst-OOV.txt\n",
      "11 C:\\Users\\ydjoe\\Documents\\nlp\\personalized-dialog-dataset\\small\\txt files\\personalized-dialog-task3-options-tst.txt\n",
      "12 C:\\Users\\ydjoe\\Documents\\nlp\\personalized-dialog-dataset\\small\\txt files\\personalized-dialog-task4-info-dev.txt\n",
      "13 C:\\Users\\ydjoe\\Documents\\nlp\\personalized-dialog-dataset\\small\\txt files\\personalized-dialog-task4-info-trn.txt\n",
      "14 C:\\Users\\ydjoe\\Documents\\nlp\\personalized-dialog-dataset\\small\\txt files\\personalized-dialog-task4-info-tst-OOV.txt\n",
      "15 C:\\Users\\ydjoe\\Documents\\nlp\\personalized-dialog-dataset\\small\\txt files\\personalized-dialog-task4-info-tst.txt\n",
      "16 C:\\Users\\ydjoe\\Documents\\nlp\\personalized-dialog-dataset\\small\\txt files\\personalized-dialog-task5-full-dialogs-dev.txt\n",
      "17 C:\\Users\\ydjoe\\Documents\\nlp\\personalized-dialog-dataset\\small\\txt files\\personalized-dialog-task5-full-dialogs-trn.txt\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,18):\n",
    "    fl_no = i\n",
    "    path_final = os.path.join(src_dir,lst_files[fl_no])\n",
    "    print(i,path_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the loop to preprocess all the similar files at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1998, 7)\n",
      "(1998, 2)\n",
      "(1998, 7)\n",
      "(1998, 2)\n",
      "(1998, 7)\n",
      "(1998, 2)\n",
      "(1998, 7)\n",
      "(1998, 2)\n"
     ]
    }
   ],
   "source": [
    "for i in range(12,16):\n",
    "    fl_no = i\n",
    "    path_final = os.path.join(src_dir,lst_files[fl_no])\n",
    "    df = convert_to_df(path_final)\n",
    "    df = add_context(df)\n",
    "    parse_from_df(df)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "540e2923c3b50c5b50181b56164d273ff885d61dafa41268e9f428c1d3e4caa0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
